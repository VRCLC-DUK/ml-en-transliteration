{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romanized Malayalam to Native Malayalam Reverse Tranliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "4Z3-WwVuBvw5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, GRU, Dense, RepeatVector, TimeDistributed, Concatenate, Dropout, LayerNormalization, Bidirectional, LSTM, GlobalAveragePooling1D, MultiHeadAttention\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Embedding, Attention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.regularizers as regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "qiHwzJ3xfh9g"
   },
   "outputs": [],
   "source": [
    "source_tokens = list('abcdefghijklmnopqrstuvwxyz ')\n",
    "source_tokenizer = Tokenizer(char_level=True, filters='')\n",
    "source_tokenizer.fit_on_texts(source_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Malayalam Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "H8G3nZJmhSrJ"
   },
   "outputs": [],
   "source": [
    "# Define Malayalam characters\n",
    "malayalam_tokens = [\n",
    "    # Independent vowels\n",
    "    'അ', 'ആ', 'ഇ', 'ഈ', 'ഉ', 'ഊ', 'ഋ', 'ൠ', 'ഌ', 'ൡ', 'എ', 'ഏ', 'ഐ', 'ഒ', 'ഓ', 'ഔ',\n",
    "\n",
    "    # Consonants\n",
    "    'ക', 'ഖ', 'ഗ', 'ഘ', 'ങ', 'ച', 'ഛ', 'ജ', 'ഝ', 'ഞ',\n",
    "    'ട', 'ഠ', 'ഡ', 'ഢ', 'ണ', 'ത', 'ഥ', 'ദ', 'ധ', 'ന',\n",
    "    'പ', 'ഫ', 'ബ', 'ഭ', 'മ', 'യ', 'ര', 'ല', 'വ', 'ശ',\n",
    "    'ഷ', 'സ', 'ഹ', 'ള', 'ഴ', 'റ',\n",
    "\n",
    "    # Chillu letters\n",
    "    'ൺ', 'ൻ', 'ർ', 'ൽ', 'ൾ',\n",
    "\n",
    "    # Additional characters\n",
    "    'ം', 'ഃ', '്',\n",
    "\n",
    "    # Vowel modifiers / Signs\n",
    "    'ാ', 'ി', 'ീ', 'ു', 'ൂ', 'ൃ', 'ൄ', 'െ', 'േ', 'ൈ', 'ൊ', 'ോ', 'ൌ', 'ൗ', ' '\n",
    "]\n",
    "\n",
    "# Create tokenizer for Malayalam tokens\n",
    "target_tokenizer = Tokenizer(char_level=True, filters='')\n",
    "target_tokenizer.fit_on_texts(malayalam_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading dataset from hugging face for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Dakshina romanized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2986,
     "status": "ok",
     "timestamp": 1733303006450,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "Onx55yLxByw6",
    "outputId": "71fd7dda-072e-4e2f-c770-0b68dbea2ef9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/socse.user06/.local/lib/python3.9/site-packages/datasets/load.py:2479: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514a7a999d9647159ea5bef0c04437c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/305 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa343b78aed4be8b3e0fcda44243952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e1fdd4ece64fcb9a0a33226f3aaacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/185673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "dataset=DatasetDict()\n",
    "#basilkr/Malasar_Dict_only\n",
    "dataset = load_dataset(\"vrclc/Dakshina-romanized-ml\",use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (185673, 2)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "artYjU-DCbt1"
   },
   "outputs": [],
   "source": [
    "data1=dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185673, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1733303009417,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "4WFZykHEniL5",
    "outputId": "4bdd87eb-8b6a-4e64-99e4-3eba3016bb41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ചാൾസ്',\n",
       " 'തടാകം',\n",
       " '1742-ൽ',\n",
       " 'എളയടത്തു',\n",
       " 'സ്വരൂപത്തെ',\n",
       " 'മാർത്താണ്ഡവർമ്മ',\n",
       " 'തിരുവിതാംകൂറിൽ',\n",
       " 'ലയിപ്പിച്ചു.',\n",
       " 'ഡെന്മാർക്ക്',\n",
       " 'ഗുരുവായൂർ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['ml'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty spaces from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To remove the identified empty rows in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_rows(example):\n",
    "    return (\n",
    "        example['ml'] is not None and example['ml'].strip() != '' and\n",
    "        example['en'] is not None and example['en'].strip() != ''\n",
    "    )\n",
    "\n",
    "# Apply the filter to the dataset\n",
    "filtered_dataset = data1.filter(filter_empty_rows)\n",
    "\n",
    "# Ensure 'ml' and 'en' are strings\n",
    "def ensure_strings(example):\n",
    "    example['ml'] = str(example['ml']) if example['ml'] is not None else ''\n",
    "    example['en'] = str(example['en']) if example['en'] is not None else ''\n",
    "    return example\n",
    "\n",
    "# Map the transformation to the dataset\n",
    "data1 = filtered_dataset.map(ensure_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ml', 'en'],\n",
       "    num_rows: 185608\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Aksharanthar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GOpQgEdJBktw"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a292bd44344ed0bbc3ea7ef84578f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7707980b92cd492093cb319ec061ad6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/276M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6811f4f19421dbb3f79167f5c7e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/400k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ea3a8626ae41cfbd2167eb3f330268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/655k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea50bb0cafc24bfbbd4400fcf58b0130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4975dda87d4847b4a6699782c0b51e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362a9104f69649979155167945b87a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"vrclc/Aksharantar-ml\",use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1733303048246,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "pnOJJjYuCr0b",
    "outputId": "4d91eaa0-3386-4b90-eef5-99182187cc44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ml', 'en', 'source'],\n",
       "        num_rows: 4100621\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ml', 'en', 'source'],\n",
       "        num_rows: 7613\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ml', 'en', 'source'],\n",
       "        num_rows: 12451\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hJ7dN0Tam7Zr"
   },
   "outputs": [],
   "source": [
    "data2=dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4100621, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty spaces from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af01e0c455c440da8eee0675f4bab148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4100621 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebe82dc337b4c8b8e8d81ef95ec8342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4100621 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dataset = data2.filter(filter_empty_rows)\n",
    "data2 = filtered_dataset.map(ensure_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ml', 'en', 'source'],\n",
       "    num_rows: 4100621\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data2.remove_columns([\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ml', 'en'],\n",
       "    num_rows: 4100621\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Dekshina Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/socse.user06/.local/lib/python3.9/site-packages/datasets/load.py:2479: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9c627ce1734f338d0cffc43eddfe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/837 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982e6eab13774764849368304dfc502d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc55451ce5049fbbc65d1f189986999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/219k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c141061abae74fbebb324d1ae6fdc929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/224k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2df87304b84072b4ee1744d65554f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83db11cf4c164a15bf6ca483b82c66f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8df0bb6a73844098be24aa5c52767bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"vrclc/dakshina-lexicons-ml\",use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ml', 'en', 'count'],\n",
       "        num_rows: 58382\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ml', 'en', 'count'],\n",
       "        num_rows: 5641\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ml', 'en', 'count'],\n",
       "        num_rows: 5610\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ml', 'en', 'count'],\n",
       "    num_rows: 58382\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty spaces from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0602d4572eb048ddab8debea6e6441a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/58382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2b3ef07f1e4cb9ace9d496ced4201a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dataset = data3.filter(filter_empty_rows)\n",
    "data3 = filtered_dataset.map(ensure_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=data3.remove_columns([\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ml', 'en'],\n",
       "    num_rows: 58381\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three dataets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml': ['ചാൾസ്',\n",
       "  'തടാകം',\n",
       "  '1742-ൽ',\n",
       "  'എളയടത്തു',\n",
       "  'സ്വരൂപത്തെ',\n",
       "  'മാർത്താണ്ഡവർമ്മ',\n",
       "  'തിരുവിതാംകൂറിൽ',\n",
       "  'ലയിപ്പിച്ചു.',\n",
       "  'ഡെന്മാർക്ക്',\n",
       "  'ഗുരുവായൂർ'],\n",
       " 'en': ['Charles',\n",
       "  'thadakom',\n",
       "  '1742-il',\n",
       "  'Ilayidathu',\n",
       "  'Swarupathe',\n",
       "  'Marthandavarma',\n",
       "  'Thiruvithamkooril',\n",
       "  'layipichu.',\n",
       "  'Denmark',\n",
       "  'Guruvayur']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml': ['തൈകളുടെ',\n",
       "  'ആഘാതം',\n",
       "  'തമ്മിലുണ്ടായ',\n",
       "  'മാറ്റിയെടുത്തു',\n",
       "  'സ്വന്തമായി',\n",
       "  'അപകടകാരികളായ',\n",
       "  'ശരിയാണെന്ന്',\n",
       "  'ചാന്ദ്',\n",
       "  'ദേവിയാണ്',\n",
       "  'നാമമാണ്'],\n",
       " 'en': ['thykalude',\n",
       "  'aakhaatham',\n",
       "  'thammillundaaya',\n",
       "  'mattiyeduthu',\n",
       "  'svanthamaayi',\n",
       "  'apakadakarikalaaya',\n",
       "  'shariyaanennu',\n",
       "  'chaandu',\n",
       "  'dheviyaan',\n",
       "  'naamamaanu']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml': ['അം',\n",
       "  'അംഗ',\n",
       "  'അംഗ',\n",
       "  'അംഗം',\n",
       "  'അംഗം',\n",
       "  'അംഗങ്ങളാണ്',\n",
       "  'അംഗങ്ങളാണ്',\n",
       "  'അംഗങ്ങളായ',\n",
       "  'അംഗങ്ങളായ',\n",
       "  'അംഗങ്ങളായി'],\n",
       " 'en': ['am',\n",
       "  'amga',\n",
       "  'anga',\n",
       "  'amgam',\n",
       "  'angam',\n",
       "  'amgangalaanu',\n",
       "  'angangalaanu',\n",
       "  'amgangalaaya',\n",
       "  'angangalaaya',\n",
       "  'amgangaayi']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets\n",
    "data = concatenate_datasets([data1, data2, data3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ml', 'en'],\n",
       "    num_rows: 4344610\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning - case change, remove all undefined tokens (numbers, puntuations etc..), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 8446,
     "status": "ok",
     "timestamp": 1733303151050,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "dP6lCUaokntl",
    "outputId": "af397f29-e987-4b1b-e05e-d8ddae8d9d95"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1906c57cdd3b47e3bc56991b140633bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4344610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_text(example):\n",
    "    example['en'] = example['en'].lower()  # Convert to lowercase\n",
    "    cleaned_en = ''.join([' ' if char not in source_tokens else char for char in example['en']]).strip()\n",
    "    cleaned_en = ' '.join(cleaned_en.split())  # Replace multiple spaces with single\n",
    "    example['en'] = cleaned_en\n",
    "    cleaned_ml = ''.join([' ' if char not in malayalam_tokens else char for char in example['ml']]).strip()\n",
    "    cleaned_ml = ' '.join(cleaned_ml.split())  # Replace multiple spaces with single\n",
    "    example['ml'] = cleaned_ml\n",
    "    return example\n",
    "\n",
    "# Apply the cleaning functions to the dataset\n",
    "cleaned_dataset = data.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml': ['ചാൾസ്',\n",
       "  'തടാകം',\n",
       "  'ൽ',\n",
       "  'എളയടത്തു',\n",
       "  'സ്വരൂപത്തെ',\n",
       "  'മാർത്താണ്ഡവർമ്മ',\n",
       "  'തിരുവിതാംകൂറിൽ',\n",
       "  'ലയിപ്പിച്ചു',\n",
       "  'ഡെന്മാർക്ക്',\n",
       "  'ഗുരുവായൂർ',\n",
       "  '',\n",
       "  'പട്ടാമ്പി',\n",
       "  'പൊന്നാനി',\n",
       "  '',\n",
       "  'പാലക്കാട്',\n",
       "  'പാതയിലെ',\n",
       "  'ഒരു',\n",
       "  'സുപ്രധാന',\n",
       "  'പട്ടണവും',\n",
       "  'തൃത്താല'],\n",
       " 'en': ['charles',\n",
       "  'thadakom',\n",
       "  'il',\n",
       "  'ilayidathu',\n",
       "  'swarupathe',\n",
       "  'marthandavarma',\n",
       "  'thiruvithamkooril',\n",
       "  'layipichu',\n",
       "  'denmark',\n",
       "  'guruvayur',\n",
       "  '',\n",
       "  'pattambi',\n",
       "  'ponnani',\n",
       "  '',\n",
       "  'palakad',\n",
       "  'paathayile',\n",
       "  'oru',\n",
       "  'supradhana',\n",
       "  'pattanavum',\n",
       "  'thrithala']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4344610, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a1c7008c0047bc80f71931a1302f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4344610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dataset = cleaned_dataset.filter(filter_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ml': ['ഗിന്നസ്',\n",
       "  'റെക്കോഡും',\n",
       "  'ഈ',\n",
       "  'സെന്റിമീറ്ററുകാരനെ',\n",
       "  'തേടിയെത്തിയിരുന്നു',\n",
       "  'നവഭവദശോകദളശയനസാരേ',\n",
       "  'സൂപ്പർടക്സ്കാർട്ടിന്റെ',\n",
       "  'ഗെയിംപ്ലേ',\n",
       "  'മരിയോ',\n",
       "  'കാർട്ട്'],\n",
       " 'en': ['guiness',\n",
       "  'recordum',\n",
       "  'ee',\n",
       "  'centimeterukarane',\n",
       "  'thediyethiyirunnu',\n",
       "  'navabhavadhashokadhalashayanasare',\n",
       "  'sooppartakskaarttinte',\n",
       "  'gaiyimple',\n",
       "  'mariyo',\n",
       "  'kaart']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4342136, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. To fix the maximum input sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1733303173420,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "4dOQUM1eE6xr",
    "outputId": "037cdcd1-8ec1-43c2-ce89-178ec8801b88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac04a008db5d4bf3b7a1f3e965e403c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4342136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of 'ml' and 'en' fields\n",
    "def calculate_lengths(example):\n",
    "    return {\n",
    "        'ml_length': len(example['ml']),\n",
    "        'en_length': len(example['en'])\n",
    "    }\n",
    "\n",
    "# Apply the function to calculate lengths\n",
    "lengths_dataset = data.map(calculate_lengths)\n",
    "\n",
    "# Extract the maximum length for both fields\n",
    "max_ml_length = max(lengths_dataset['ml_length'])\n",
    "max_en_length = max(lengths_dataset['en_length'])\n",
    "\n",
    "# Calculate the maximum sequence length overall\n",
    "max_seq_length = max(max_ml_length, max_en_length)\n",
    "\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(max_ml_length)\n",
    "print(max_en_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set input lenth to max_seq_length by padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Rk97Jssi1iX0"
   },
   "outputs": [],
   "source": [
    "source_sequences = source_tokenizer.texts_to_sequences(data['en'])\n",
    "X = pad_sequences(source_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "target_sequences = target_tokenizer.texts_to_sequences(data['ml'])\n",
    "y = pad_sequences(target_sequences, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4342136, 57)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train - Validation data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "vO1jr876ExF4"
   },
   "outputs": [],
   "source": [
    "# Train-Validation-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1733303232495,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "QQpfXx_65hZx",
    "outputId": "c1036759-565a-431f-dd53-c9b527d7a02e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3907922, 57)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1733303248462,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "hBCaT03p5s1X",
    "outputId": "5316871b-7f28-4007-f31e-694a5795d026"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434214, 57)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Defenition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "OmqS-iJyFSXp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 14:08:26.271098: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Define the model parameters\n",
    "embedding_dim = 64\n",
    "hidden_units = 128  # Adjusted to match both encoder and decoder\n",
    "#max_seq_length = max(data['ml'].apply(len).max(), data['en'].apply(len).max())\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=(max_seq_length,), name='encoder_input')\n",
    "encoder_embedding = Embedding(input_dim=len(source_tokenizer.word_index) + 1, output_dim=embedding_dim, name='encoder_embedding')(encoder_input)\n",
    "encoder_lstm = Bidirectional(LSTM(hidden_units, return_sequences=True, name='encoder_lstm'))(encoder_embedding)\n",
    "\n",
    "# Adjust encoder output for attention compatibility\n",
    "encoder_lstm_adjusted = Dense(hidden_units, activation='relu', name='encoder_dense_adjust')(encoder_lstm)\n",
    "\n",
    "# Decoder\n",
    "decoder_input = RepeatVector(max_seq_length, name='decoder_repeat_vector')(encoder_lstm_adjusted[:, -1, :])  # Use last state for RepeatVector\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, name='decoder_lstm')(decoder_input)\n",
    "\n",
    "# Apply attention between the encoder outputs and decoder outputs\n",
    "attention_output = Attention(name='attention_layer')([decoder_lstm, encoder_lstm_adjusted])\n",
    "decoder_concat = Concatenate(axis=-1, name='decoder_concat')([decoder_lstm, attention_output])\n",
    "decoder_dense = TimeDistributed(Dense(len(target_tokenizer.word_index) + 1, activation='softmax', name='decoder_output_dense'))(decoder_concat)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=encoder_input, outputs=decoder_dense)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Fb5vUrY9PW4x"
   },
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('leenag/transliteration', monitor='val_loss', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374467,
     "status": "ok",
     "timestamp": 1733305773459,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "SPb0QwhNLJe4",
    "outputId": "f9e4ce42-389f-49c2-f5e9-2cb6c634e5c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39969/61062 [==================>...........] - ETA: 39:09 - loss: 0.1248 - accuracy: 0.9610"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, np.expand_dims(y_train, -1),\n",
    "    validation_data=(X_val, np.expand_dims(y_val, -1)),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1733306502745,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "Y7iskuKVT30R",
    "outputId": "c67d8605-3ad7-4162-97f5-c5913184ef3c"
   },
   "outputs": [],
   "source": [
    "# '''  ''' Plot training history\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "pBuGq0kMFTLR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: leenag/transliteration/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: leenag/transliteration/assets\n"
     ]
    }
   ],
   "source": [
    "# Assuming you are using a Keras model\n",
    "model.save('leenag/transliteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 57)]                 0         []                            \n",
      "                                                                                                  \n",
      " encoder_embedding (Embeddi  (None, 57, 64)               1792      ['encoder_input[0][0]']       \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 57, 256)              197632    ['encoder_embedding[0][0]']   \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_dense_adjust (Dens  (None, 57, 128)              32896     ['bidirectional[0][0]']       \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 128)                  0         ['encoder_dense_adjust[0][0]']\n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " decoder_repeat_vector (Rep  (None, 57, 128)              0         ['tf.__operators__.getitem[0][\n",
      " eatVector)                                                         0]']                          \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         (None, 57, 128)              131584    ['decoder_repeat_vector[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " attention_layer (Attention  (None, 57, 128)              0         ['decoder_lstm[0][0]',        \n",
      " )                                                                   'encoder_dense_adjust[0][0]']\n",
      "                                                                                                  \n",
      " decoder_concat (Concatenat  (None, 57, 256)              0         ['decoder_lstm[0][0]',        \n",
      " e)                                                                  'attention_layer[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 57, 76)               19532     ['decoder_concat[0][0]']      \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 383436 (1.46 MB)\n",
      "Trainable params: 383436 (1.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "so6pkXJbiDHx"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Load the saved model\n",
    "model = load_model('leenag/transliteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a romanized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "abq1IsiecOlR"
   },
   "outputs": [],
   "source": [
    "# prompt: Rewrite the above code to split the input_text by tokens and non-tokens. Pass the tokens to the model and get the output, retain the non-tokens as such in the output\n",
    "\n",
    "import re\n",
    "\n",
    "def transliterate_with_non_tokens(input_text, model, source_tokenizer, target_tokenizer, max_seq_length):\n",
    "    \"\"\"\n",
    "    Transliterates input text, preserving non-token characters.\n",
    "    \"\"\"\n",
    "    # Regular expression to split the text into tokens and non-tokens\n",
    "    tokens_and_non_tokens = re.findall(r\"([a-zA-Z]+)|([^a-zA-Z]+)\", input_text)\n",
    "\n",
    "    transliterated_text = \"\"\n",
    "    for token_or_non_token in tokens_and_non_tokens:\n",
    "        token = token_or_non_token[0]\n",
    "        non_token = token_or_non_token[1]\n",
    "\n",
    "        if token:\n",
    "            input_sequence = source_tokenizer.texts_to_sequences([token])[0]\n",
    "            input_sequence_padded = pad_sequences([input_sequence], maxlen=max_seq_length, padding='post')\n",
    "            predicted_sequence = model.predict(input_sequence_padded)\n",
    "            predicted_indices = np.argmax(predicted_sequence, axis=-1)[0]\n",
    "            transliterated_word = ''.join([target_tokenizer.index_word[idx] for idx in predicted_indices if idx != 0])\n",
    "            transliterated_text += transliterated_word\n",
    "        elif non_token:\n",
    "            transliterated_text += non_token\n",
    "\n",
    "    return transliterated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1733306190274,
     "user": {
      "displayName": "Kavya Manohar",
      "userId": "18106355877312530441"
     },
     "user_tz": -330
    },
    "id": "gQ4rMJCweZRf",
    "outputId": "5a094ad3-9a82-4be9-e8dc-6f165101919b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted Transliteration: പ്രഭാതത്തിലെ ഉറക്കത്തിന് ഒരു വല്ലാത്ത സുഖമാണ്\n"
     ]
    }
   ],
   "source": [
    "input_text = \"prabhathathile urakkathinu oru vallaatha sukhamanu\"\n",
    "transliterated_text = transliterate_with_non_tokens(input_text, model, source_tokenizer, target_tokenizer, max_seq_length)\n",
    "print(\"Predicted Transliteration:\", transliterated_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1ueKqhbM7ofpzGfMdRowMl5oqCdvYIaDb",
     "timestamp": 1733326933715
    },
    {
     "file_id": "1nWMzgJS1TEA0cWi_k_yU4fcIbDsrw4lC",
     "timestamp": 1733217895061
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
